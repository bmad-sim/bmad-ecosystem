\chapter{Optimization: Lattice Correction and Design}
\label{c:opti}

\index{optimization|hyperbf}

``Optimization'' is the process of varying model (\sref{s:lattice}) lattice parameters to create a
lattice with a certain set of properties as close to a ``desired'' state as possible. Optimization
problems generally fall into one of two categories. One category involves ``lattice
correction''. Problems in this category involve matching the \vn{model} lattice to actual measured
data.  For example, orbit flattening involves varying steering in the \vn{model} lattice so that the
orbit calculated from the \vn{model} lattice matches as close as possible the measured orbit. The
steering strengths in the \vn{model} lattice can then be used to calculate what changes are needed
to correct the orbit in the actual machine. This is discussed in
Section~\sref{s:lattice.correction}.

The other category of optimization problems involves ``lattice design'' where lattice parameters are
varied to achieve some set of ideal properties. For example, varying sextupole magnet strengths in
order to give maximum dynamic aperture. This is discussed in Section~\sref{s:lattice.design}.


%------------------------------------------------------------------------
\section{Lattice Corrections}
\label{s:lattice.correction}
\index{modeling data}
\index{lattice corrections}

\index{optimization!merit function}
Consider the problem of modifying the orbit of a beam through a lattice to conform to some desired
orbit (typically this is the ``flat'' orbit running through the centers of the quadrupoles). The
process generally has three stages: First the orbit is measured, then corrections to the steering
elements are calculated (the optimization stage), and finally the corrections are applied to the
machine. Since measurement and correction loading necessarily need communication with machine
hardware, \tao has no native ability to do these tasks.

but they could be implemented with some custom coding as discussed in Chapter~\sref{c:custom.tao}.
What \tao does, however, is to implement a generalized algorithm procedure for minimizing a
\vn{merit function} which can be used to calculate the corrections.  The idea is to vary a set of
variables (steerings in the case of an orbit correction) within the \vn{model} lattice
(\sref{s:universe}) with the aim to make the \vn{measured} \vn{data} (position data for an orbit
correction) correspond to the values as calculated from the \vn{model} lattice.  Once the model
lattice the \vn{model} and \vn{measured} data agree, the difference between the \vn{model}, which
represents the state of the machine when the measurement is made, and the \vn{design}, which
represents the desired state of the machine, is used to calculate corrections. In the case of
flattening an orbit, the difference between the \vn{model} steering strengths and the \vn{design}
steering strengths (typically the \vn{design} steering strengths are zero) is what the real
steerings need to be changed by to flatten the orbit.

The merit function \vn{M} that is a measure of how well the data as calculated from the \vn{model},
fits the measured data. \tao uses a merit function of the form
\begin{equation}
  {\cal M} \equiv 
    \sum_{i} w_i \, \bigl[ \delta D_i \bigr]^2 + 
    \sum_{j} w_j \, \bigl[ \delta V_j \bigr]^2
  \label{m1}
\end{equation}
where
\begin{align}
  \delta D &= \data\_\model - \data\_\meas \CRNO
  \delta V &= \var\_\model  - \var\_\meas
  \label{dd1}
\end{align}
\vn{data_model} is the data as calculated from the \vn{model} and \vn{data_meas} is the measured
data. \vn{var_model} is the value of a variable in the \vn{model} and \vn{var_meas} is the value as
measured at the time the data was taken (for example, by measuring the current through a steering
and using a calibration factor to calculate the kick) and the sum \vn{j} runs over all variables
that are allowed to be varied to minimize \vn{M}. The second term in the merit function prevents
degeneracies (or near degeneracies) in the problem which would allow \tao to find solutions where
\vn{data_model} matches \vn{data_measured} with the \vn{var_model} having ``unphysical'' values
(values far from \vn{var_meas}). The weights $w_i$ and $w_j$ need to be set depending upon how
accurate the measured data is relative to how accurate the calibrations for measuring the
\vn{var_meas} values are. With the second term in the merit function, the number of constraints
(number of terms in the merit function) is always larger than the number of variables and
degeneracies can never occur.

In a correction, one wants to change the machine variables so that the measured data corresponds to
the design values \vn{data_design}. For example, when ``flattening'' the orbit, \vn{data_design}
will be zero. The desired change in the data when the machine variables are varied is
\begin{example}
  desired_data_change = data_design - data_meas
\end{example}
Once a fit has been made, and presuming that the \vn{data_model} is reasonably close to the
\vn{data_meas} this desired data change within the \vn{model} lattice can be accomplished by
changing the model variables by
\begin{example}
  var_change = var_design - var_model
\end{example}
This assumes the system is linear. For many situations this is true since typically \vn{var_change}
is ``small''. Since the variables have a measured value of \vn{var_meas} the value that the
variables should be set to is
\begin{example}
  var_final = var_meas + (var_design - var_model)
\end{example}
Notice that the fitting process is independent of the \vn{design} lattice. It is only when
calculating the corrections to the variables that the \vn{design} lattice plays a role.

Sometimes it is desired to fit to changes in data as opposed to the absolute value of the
data. For example, when closing an orbit bump knob what is important is the difference in
orbits before and after the bump knob is varied. Designating one of these orbit the
\vn{reference}, the appropriate deltas to be used in \Eq{m1} are
\begin{align}
  \delta D &= (\data\_\model - \data\_\design) - (\data\_\meas - \data\_\reference) \CRNO
  \delta V &= (\var\_\model - \var\_\design)   - (\var\_\meas - \var\_\reference)
  \label{dd2}
\end{align}
where \vn{data_ref} and \vn{var_ref} refer to the reference measurement.  These deltas
are acceptable if the reference data is taken with the machine reasonably near the
design setup so that nonlinearities can be ignored. If this is not the case then the
fitting becomes a two step process: The first step is to fit the \vn{model} to the
\vn{reference} data using the deltas of \Eq{dd1}. The \vn{base} lattice is then set
equal to the \vn{model} lattice. The second step is to fit the model using the deltas
\begin{align}
  \delta D &= (\data\_\model - \data\_\base) - (\data\_\meas - \data\_\reference) \CRNO
  \delta V &= (\var\_\model - \var\_\base)   - (\var\_\meas - \var\_\reference)
  \label{dd3}
\end{align}

Control of what data and what variables are to be used in the fitting process is controlled by the
\vn{use}, \vn{veto}, \vn{restore}, and \vn{clip} commands.

%------------------------------------------------------------------------
\section{Lattice Design}
\label{s:lattice.design}
\index{optimization!lattice design}
\index{optimization!constraints}

Lattice design is the process of calculating \vn{variable} strengths to meet a number of criteria
called constraints. For example, one constraint could be that the beta function in some part of the
lattice not exceed a certain value. In this case, we can proceed as was done for lattice corrections
and use \Eq{m1}. Here the deltas are computed to limit values to some range so a 
delta would be of the form
\begin{equation}
  \delta D \; \text{or} \; \delta V = 
    \begin{cases}
    \mbox{model} - \mbox{limit}  & \mbox{model $>$ Limit} \\
    0                            & \mbox{otherwise}
    \end{cases}
\end{equation}
or a constraint is used to keep the \vn{model} at a certain value so the form of the constraint
would be
\begin{equation}
    \delta D \; \text{or} \; \delta V = \mbox{model} - \mbox{target}  
\end{equation}
Here \vn{model} is the value as calculated from the \vn{model} lattice. \vn{target} and \vn{limit}
are given by the user. Part of the optimization process is in deciding what the values should be for
any \vn{target} or \vn{limit}.

%------------------------------------------------------------------------
\section{Generalized Design}
\label{s:generalized.design}
\index{optimization!generalized merit function}

The form of the deltas used in the merit function is determined by two global logicals called
\vn{opt_with_ref} and \vn{opt_with_base} (\sref{s:globals}) as shown in Table~\ref{t:delta}.
\begin{table}[ht] 
\centering 
{\tt
\begin{tabular}{lll} \toprule
  \vn{Opt_with_ref} & \vn{Opt_with_base} & \vn{delta} \\ \midrule
  F & F & model - meas                \\
  T & F & model - meas + ref - design \\
  F & T & model - meas - base         \\
  T & T & model - meas + ref - base   \\
\bottomrule
\end{tabular}
} 
\caption{The form of \vn{delta}}  
\label{t:delta}
\end{table}
An exception occurs when using a \vn{common root lattice} (\sref{s:crl}). In this case, the common
universe does not have base or reference values associated with it. Thus all data and variables that
are associated with the common universe calculate their \vn{delta} as if both \vn{opt_with_ref} and
\vn{opt_with_base} were set to \vn{False}.

Another exception occurs with data when the datum value cannot be computed (\sref{s:datum.opt}). In
this case, the datum's \vn{invalid} value is used for the \vn{delta}. This is useful, for example,
in a linear lattice when the particle trajectory results in the particle being lost.

The \vn{Non-Zero-Condition} needed for a non--zero $D_i$ is dependent upon the \vn{merit_type} of
the datum (\sref{s:data.anatomy}).  There are seven \vn{merit_type} constraint types as given in
Table~\ref{t:con.type}.
\begin{table}[ht]
\centering
{\tt
\begin{tabular}{|l|l|l|} \toprule
  {\it Merit\_Type}         & {\it Non-zero-Condition} \\ \midrule
  \vn{target}, \vn{average} & Any \vn{delta}   \\
  \vn{max-min}              & Any \vn{delta}   \\
  \vn{min}, \vn{abs_min}    & \vn{delta} $<$ 0 \\
  \vn{max}, \vn{abs_max}    & \vn{delta} $>$ 0 \\
\bottomrule
\end{tabular}
}
\caption{Data Constraint Type List.}
\label{t:con.type}
\end{table}
\index{optimization!optimize with reference}

For variables, the form of the terms $V_i$ is determined by its \vn{merit_type}.  Here the
\vn{merit_type} may be:
\begin{example}
  target
  limit
\end{example}
A \vn{target} \vn{merit_type} for a variable is the same as for datum. In this case, \vn{model} is
just the value of the variable.  

A \vn{limit} \vn{merit_type} has the form
\begin{equation}
  \delta V = 
    \begin{cases}
    \mbox{model} - \mbox{high\_lim}  & \mbox{model} > \mbox{high\_lim} \\
    \mbox{model} - \mbox{low\_lim}   & \mbox{model} < \mbox{low\_lim} \\
    0                                & \mbox{Otherwise}
    \end{cases}
\end{equation}
The default \vn{merit_type} for a variable is \vn{limit}.

Note: when doing lattice design, \vn{opt_with_ref} and \vn{opt_with_base} are both set to \vn{False}
and the \vn{target} and \vn{limit} values are identified with \vn{Meas}.

When optimizing a storage ring, If the ring is unstable so that the twiss parameters, closed orbit,
etc. cannot be computed, the contribution to the merit function from the corresponding datums is set
to zero. This tends to lower the merit function and in this case, an optimizer will never leave the
unstable region. To avoid this, an \vn{unstable.lattice} constraint (\sref{s:data.types}) must be set.

To see a list of constraints when running \tao use the \vn{show constraints} command
(\sref{s:show}). To see how a particular variable or datum is doing use the \vn{show data} or
\vn{show variable} commands.  See \sref{s:datum.opt} for details on how datums are chosen to be
included in an optimization.

%------------------------------------------------------------------------
\section{Variable Limits and Optimization}
\label{s:limit}

High (\vn{high_lim}) and low (\vn{low_lim}) limiting values can be set for any variable
(\sref{s:init.var}). If not explicitly set, \vn{high_lim} defaults to $10^{30}$ and \vn{low_lim}
defaults to $-10^{30}$. When running the optimizer, if the parameter \vn{global%var_limits_on}
(\sref{s:globals}) is \vn{True}, and if the (model) value of a variable is outside of
the range set by the limits, the value will be set to the value of the appropriate limit and the
variable's \vn{good_user} parameter (\sref{c:var}) is set to False so that no further variation by
the optimizer is done.

By default, any variable value outside of the limit range will reset. Even those variables that are
not varied by the optimizer. If this behavior is not desired, the parameter
\vn{global%only_limit_opt_vars} may be set to \vn{True}.  If this is done, only variables that the
optimizer is allowed to vary are restricted.

The \vn{global%optimizer_var_limit_warn} parameter controls whether a warning is printed when a
variable value goes past a limit.  The default is \vn{True}.

%------------------------------------------------------------------------
\section{Optimizers in Tao}
\label{s:tao.opti}
\index{optimization!optimizer}

The algorithm used to vary the \vn{model} variables to minimize \vn{M} is called an \vn{optimizer}.
In \vn{command line mode} the \vn{run} command is used to invoke an \vn{optimizer}. In \vn{single
mode} the \vn{g} key starts an optimizer. In both modes the period key (\vn{``.''}) stops the
optimization (however, the \vn{global%optimizer_allow_user_abort} parameter (\sref{s:globals}) can
be set to False to prevent this). Running an optimizer is also called ``fitting'' since one is
trying to get the \vn{model} data to be equal to the \vn{measured} data. With orbits this is also
called ``flattening'' since one generally wants to end up with an orbit that is on--axis.

The optimizer that is used can be defined when using the \vn{run} command but
the default optimizer can be set in the \tao input file by setting the
\vn{global%optimizer} component (\sref{s:globals}).

When the optimizer is run in \tao, the optimizer, after it initializes
itself, takes a number of \vn{cycles}. Each cycle consists of changing
the values of the variables the optimizer is allowed to change. The
number of steps that the optimizer will take is determined by the
parameter \vn{global%n_opti_cycles} (\sref{s:globals}). When the
optimizer initializes itself and goes through
\vn{global%n_opti_cycles}, it is said to have gone through one
\vn{loop}. After going through through \vn{global%n_opti_loops} loops,
the optimizer will automatically stop.  To immediately stop the
optimizer the period key \vn{``.''} may be pressed. Note: In
\vn{single_mode} (\sref{c:single}), \vn{n_opti_loops} is ignored and
the optimizer will loop forever.

There are currently three optimizers that can be used: 
  \begin{description}
  \index{lm optimizer}
  \item{\vn{lm}} \Newline
\vn{lm} is an optimizer based upon the Levenburg-Marquardt algorithm
as implemented in \vn{Numerical Recipes}\cite{b:nr}. This algorithm
looks at the local derivative matrix of \vn{dData/dVariable} and takes
steps in variable space accordingly. The derivative matrix is
calculated beforehand by varying all the variables by an amount set by
the variable's \vn{step} component (\sref{s:init.var}). The \vn{step}
size should be chosen large enough so that round-off errors will not
make computation of the derivatives inaccurate but the step size
should not be so large that the derivatives are effected by
nonlinearities. By default, the derivative matrix will be recalculated
each \vn{loop} but this can be changed by setting the
\vn{global%derivative_recalc} global parameter (\sref{s:globals}). The
reason to not recalculate the derivative matrix is one of time.
However, if the calculated derivative matrix is not accurate (that is,
if the variables have changed enough from the last time the matrix was
calculated and the nonlinearities in the lattice are large enough),
the \vn{lm} optimizer will not work very well.  In any case, this
method will only find local minimum.
  \index{lmdif optimizer}
  \item{\vn{lmdif}} \Newline
The \vn{lmdif} optimizer is like the \vn{lm} optimizer except that it
builds up the information it needs on the derivative matrix by
initially taking small steps over the first \vn{n} cycles where \vn{n}
is the number of variables. The advantage of this is that you do not
have to set a \vn{step} size for the variables. The disadvantage is
that for \vn{lmdif} to be useful, the number of \vn{cycles} must be
greater than the number of variables. Again, like \vn{lm}, this method
will only find local minimum.
  \index{de optimizer}
  \item{\vn{de}} \Newline
The \vn{de} optimizer stands for \vn{differential
evolution}\cite{b:de}. The advantage of this optimizer is that it
looks for global minimum. The disadvantage is that it is slow to find
the bottom of a local minimum. A good strategy sometimes when trying
to find a global minimum is to use \vn{de} in combination with \vn{lm}
or \vn{lmdif} one after the other. One important parameter with the
\vn{de} optimizer is the \vn{step} size. A larger step size means that
the optimizer will tend to explore larger areas of variable space but
the trade off is that this will make it harder to find minimum in the
locally. One good strategy is to vary the \vn{step} size to see what
is effective. Remember, the optimal step size will be different for
different problems and for different starting points. The \vn{step}
size that is appropriate of the \vn{de} optimizer will, in general, be
different from the \vn{step} size for the \vn{lm} optimizer. For this
reason, and to facilitate changing the step size, the actual step size
used by the \vn{de} optimizer is the step size given by a variable's
\vn{step} component multiplied by the global variable
\vn{global%de_lm_step_ratio}. This global variable can be varied using
the \vn{set} command (\sref{s:set}). The number of trial solutions used 
in the optimization is
\begin{example}
  population = number_of_variables * global%de_var_to_population_factor
\end{example}
There are also a number of parameters that can be set that will affect
how the optimizer works. See Section~\sref{s:globals} for more
details.
  \index{svd optimizer}
  \item{\vn{svd}} \Newline
The \vn{svd} optimizer uses a singular value decomposition
calculation.  See the description of \vn{svdfit} from Numerical
Recipes\cite{b:nr} for more details. With the \vn{svd} optimizer, the
setting of the \vn{global%n_opti_cycles} parameter is ignored. One
optimization loop consists of applying svd to the derivative matrix to
locate a new set of variable values.  If the merit function decreases
with the new set, the new values are retained and the optimization
loop is finished. If the merit function increases, and if the global
variable \vn{global%svd_retreat_on_merit_increase} is True (the
default), the variables are set to the original variable settings. In
either case, an increasing merit function will stop the execution of
additional loops.

The \vn{global%svd_cutoff} variable can be used to vary the cutoff
that SVD uses to decide what eigenvalues are sigular. See the
documentation for the Numerical Recipes routine \vn{svdfit} for more
details.
  \end{description}

%------------------------------------------------------------------------
\section{Optimization Troubleshooting Tips}
\label{s:opt.trouble}
\index{optimization troubleshooting}

Optimizations can behave in strange ways. Here are some tips on how to diagnose problems.

The \vn{show optimizer} (\sref{s:show.optimizer}) command will show global parameters
associated with optimizations. This will show some of the parameters that can be 
varied to get better convergence. One quick thing to do is to increase the number of
optimization loops and/or optimization cycles:
\begin{example}
	set global n_opti_loops = ...
	set global n_opti_cycles = ...
\end{example}

One of the first things to check is the merit function, the top contributors can be seen
with the command \vn{show merit} (\sref{s:show.merit}). And individual contributions
can be viewed using the \vn{show variable} and \vn{show data} commands.

If using an optimizer that uses the derivative matrix (\vn{lm}, \vn{geodesic_lm} and 
\vn{svd} optimizers), The variable \vn{step} sizes that are used to calculate the derivative
should be checked to make sure that the \vn{step} is not too small so that roundoff is a problem
but yet not too large so that nonlinearities make the calculation inaccurate. One way to
check that the step size is adequate for a given variable is to vary the variable using
the command \vn{change var} (\sref{s:change}). This command will print out the the change
in the merit function per change in variable which can be compared to the derivatives 
as shown with the \vn{show merit -derivative} (\sref{s:show.merit}) or the
\vn{show derivative} (\sref{s:show.derivative}) command.

%------------------------------------------------------------------------
\section{Common Root Lattice (CRL) Analysis}
\label{s:crl}
\index{common root lattice}

Some data analysis problems involve varying variables in a both the
\vn{model} and \vn{base} lattices simultaneously. Such is the case
with Orbit Response Matrix (\vn{ORM}) analysis\cite{b:orm}. With
\vn{ORM}, the analysis starts with a set of difference orbits. A given
difference orbit is generated by varying a given steering by a known
amount and the steering varied is different for different difference
orbits. Typically, The number $N$ of difference orbits is equal to the
number of steering elements in the machine. In \tao, this will result
in the creation of $N$ universes, one for each difference
measurement. The \vn{model} lattice in a universe will correspond to
the machine with the corresponding steering set to what it was when
the data was taken. Conversely, the \vn{base} lattices in all the
universes all correspond to the common condition without any steering
variation.

In \tao, this arrangement is called \vn{Common Root Lattice}
(\vn{CRL}) analysis. To do a CRL analysis, the \vn{common_lattice}
switch must be set at initialization time (\sref{s:init.lat}).  With
\vn{CRL}, \tao will set up a \vn{``common''} universe with index 0.
The \vn{model} lattice of this common universe will be used as the
\vn{base} lattice for all universes. 

The variables (fit parameters) in a \vn{CRL} analysis can be divided
into two classes. One class consists of the parameters that were
varied to get the data of the different universes. With \vn{ORM},
these are the steering strengths. At initialization
(\sref{s:init.var}), variables must be set up that control these
parameters. A single variable will control that particular parameter in
a particular universe, that was varied to create the data for that
universe. 

The second class of variables consists of everything that is
to be varied in the common root lattice. With \vn{ORM}, this generally
will include such things as quadrupole and BPM error tilts, etc. That
is, parameters that did {\em not} change during data taking. The
\tao variables that are created for these parameters will control
parameters of the \vn{model} lattice in the common universe.

To cut down on memory usage when using \vn{CRL} (the number of data
sets, hence the number of universes, can be very large), \tao does
not, except for the common \vn{model} lattice, reserve separate memory
for each \vn{model} lattice. Rather, it reserves memory for a single
\vn{``working''} lattice and the \vn{model} lattice for a particular
universe is created by first copying the common \vn{base} lattice to
the \vn{working} lattice and then applying the variable(s) (a steering
in the case of \vn{ORM}) appropriate for that universe.  As a result,
except for the common \vn{model} lattice, it is not possible to vary a
parameter of a \vn{model} lattice unless that parameter has a \tao
variable that associated with it. The \vn{change} command
(\sref{s:change}) is thus restricted to always vary parameters in
the common \vn{model} lattice.

With \vn{CRL}, the \vn{opt_with_base} and \vn{opt_with_ref}
(\sref{s:generalized.design}) global logicals are generally set to
True. Since \vn{opt_with_base}, and \vn{opt_with_ref} do not make
sense when applied to the data in the common universe, The
contribution to the merit function from data in this universe is
always calculated as if \vn{opt_with_base} and \vn{opt_with_ref} were
set to False.

With \vn{opt_with_base} set to True, the \vn{base} value for a
datum is evaluated by looking for a corresponding datum in the common
universe and using its \vn{model} value. To simplify the bookkeeping,
it is assumed that the structure of the data arrays is identical from
universe to universe. That is, the \vn{show data} command gives
identical results independent of the default universe. 
